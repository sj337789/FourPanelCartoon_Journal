# Web Crawling script for Chosun News Library (조선일보 뉴스 라이브러리: https://newslibrary.chosun.com/)
# This script was executed on the PyCharm software on the Windows 10 Operating System.

import os
import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor
from PIL import Image

data = pd.read_csv('C:/Users/User/Desktop/chosun_1920to1940_link.csv', header=None)
image_urls = data[0].tolist()

destination_folder = 'E:/jpg_1920_1940_chosun_images/'
os.makedirs(destination_folder, exist_ok=True)

def download_image(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        file_name = url.rsplit("/", 1)[-1]
        file_path = os.path.join(destination_folder, file_name)
        with open(file_path, "wb") as f:
            f.write(response.content)
        print(f"Downloaded: {url}")
    except requests.exceptions.RequestException as e:
        print(f"Failed to download {url}. Error: {e}")

with ThreadPoolExecutor(max_workers=200) as executor:
    executor.map(download_image, image_urls)
